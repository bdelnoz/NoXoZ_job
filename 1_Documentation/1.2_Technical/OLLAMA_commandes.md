---
# **üìú Commandes Ollama Compl√®tes (Bruno Delnoz - Kali Linux)**
**Auteur** : Bruno Delnoz (nox@casablanca)
**Date** : 2026-02-05
**Version** : 1.0
**Format** : Markdown (`.md`)
**Emplacement** : `/mnt/data2_78g/Security/scripts/Projects_system/Docs/commandes_ollama.md`
**Contexte** : Scripts pour Kali Linux, format Markdown strict, sans num√©ros de version dans les noms de fichiers.
---

```bash
################################################################################
# 1Ô∏è‚É£ GESTION DES MOD√àLES #######################################################
################################################################################

# T√©l√©chargement/Mise √† jour
ollama pull <mod√®le>                     # Ex: ollama pull phi3:3.8b (128K tokens, id√©al pour 350 questions)
ollama pull --help                        # Affiche l'aide compl√®te
ollama pull <mod√®le> --show-progress      # Affiche la progression du t√©l√©chargement
ollama pull --update <mod√®le>             # Met √† jour un mod√®le existant

# Liste/Suppression
ollama list                              # Liste tous les mod√®les locaux
ollama rm <mod√®le>                       # Supprime un mod√®le sp√©cifique (ex: ollama rm mistral:7b)
ollama rm --all                          # ‚ö†Ô∏è Supprime TOUS les mod√®les locaux (√† utiliser avec prudence)

################################################################################
# 2Ô∏è‚É£ EX√âCUTION ET INTERACTION ###################################################
################################################################################

# Mode Interactif
ollama run <mod√®le>                      # Lance un mod√®le en mode conversationnel (ex: ollama run llama3:8b)
ollama run <mod√®le> "<prompt>"           # Ex√©cute une requ√™te unique (ex: ollama run phi3:3.8b "Explique Docker")

# Options Avanc√©es
ollama run <mod√®le> --verbose             # Active le mode verbeux (logs d√©taill√©s)
ollama run <mod√®le> --temperature 0.8    # Contr√¥le la cr√©ativit√© (0=pr√©cis, 1=cr√©atif)
ollama run <mod√®le> --num-gpu 1           # Utilise 1 GPU pour l'inf√©rence
ollama run <mod√®le> --num-threads 8       # Limite √† 8 threads CPU
ollama run <mod√®le> --mirostat 2          # Am√©liore la coh√©rence des r√©ponses
ollama run <mod√®le> --repeat_penalty 1.2  # R√©duit les r√©p√©titions dans les r√©ponses

################################################################################
# 3Ô∏è‚É£ SERVEUR OLLAMA ###########################################################
################################################################################

# D√©marrage/Arr√™t
ollama serve                             # D√©marre le serveur Ollama (port 11434 par d√©faut)
ollama serve --host 0.0.0.0              # ‚ö†Ô∏è Autorise les connexions externes (attention s√©curit√©)
ollama serve --port 11435                # Change le port par d√©faut
pkill -f ollama                          # Arr√™te le serveur (Kali Linux)

# Configuration
export OLLAMA_MODELS=/mnt/data1_100g/agent_llm_local/models  # Chemin personnalis√© pour les mod√®les
export OLLAMA_HOST=0.0.0.0:11434         # D√©finit l'h√¥te et le port
ollama serve --debug                     # Active le mode debug (logs √©tendus)

################################################################################
# 4Ô∏è‚É£ MODELFILES (Personnalisation) #############################################
################################################################################

# Exemple de Modelfile pour un assistant Kali Linux (√† enregistrer sous kali_assistant.modelfile)
# FROM llama2:7b
# PARAMETER temperature 0.8
# SYSTEM "Expert en Kali Linux, sp√©cialis√© en s√©curit√© et scripts shell. R√©ponds de mani√®re concise et technique."

# Cr√©ation du mod√®le personnalis√©
ollama create kali_assistant -f kali_assistant.modelfile

################################################################################
# 5Ô∏è‚É£ API ET R√âSEAU #############################################################
################################################################################

# Requ√™tes API
curl http://localhost:11434/api/tags      # Liste les mod√®les disponibles via l'API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3:8b", "prompt": "Bonjour, explique-moi comment configurer un pare-feu sous Kali Linux"}'

# S√©curit√© R√©seau
ollama serve --tls-key key.pem --tls-cert cert.pem  # Active le chiffrement TLS

################################################################################
# 6Ô∏è‚É£ PERFORMANCES ET D√âBOGAGE ##################################################
################################################################################

# Optimisation des performances
ollama run <mod√®le> --mirostat 2          # Meilleure coh√©rence des r√©ponses
ollama run <mod√®le> --repeat_penalty 1.2  # R√©duit les r√©p√©titions

# D√©bogage
journalctl -u ollama -f                  # Affiche les logs en temps r√©el (systemd)
ollama run <mod√®le> --verbose             # Active le mode verbeux pour un mod√®le sp√©cifique

################################################################################
# 7Ô∏è‚É£ SCRIPTS PR√äTS √Ä L'EMPLOI ###################################################
################################################################################

# Script 1: Analyser un fichier texte (ex: 350 questions)
#!/bin/bash
# Usage: ./analyse_doc.sh mon_fichier.txt "Ta question"
ollama run phi3:3.8b "Analyse ce fichier : \$(cat \$1) et r√©ponds √† : \$2" > "analyse_\$(date +%Y%m%d).md"

# Script 2: G√©n√©rer une synth√®se
#!/bin/bash
# Usage: ./synthese_doc.sh mon_fichier.txt
ollama run llama3:8b "R√©sume ce document en 5 points cl√©s : \$(cat \$1)" > "synthese_\$(date +%Y%m%d).md"

# Script 3: Lister les mod√®les (filtr√©)
ollama list | grep -E "NAME|phi3:3.8b"     # Filtre un mod√®le sp√©cifique

################################################################################
# 8Ô∏è‚É£ MOD√àLES RECOMMAND√âS (Bruno Delnoz) ########################################
################################################################################

# | Mod√®le          | Taille (Go) | Tokens Max | Usage Recommand√©                          | Commande               |
# |-----------------|-------------|------------|-------------------------------------------|-------------------------|
# | phi3:3.8b       | 2.3          | 128,000    | Documents tr√®s longs (350 questions)       | ollama pull phi3:3.8b   |
# | mistral:7b      | 4.1          | 8,192      | Scripts Kali Linux                       | ollama pull mistral:7b  |
# | codellama:34b   | 20           | 32,000     | Analyse de code                          | ollama pull codellama:34b|
# | dbrx:132b       | 240          | 128,000    | Analyse de tr√®s longs documents          | ollama pull dbrx:132b   |

################################################################################
# 9Ô∏è‚É£ ERREURS COURANTES ET SOLUTIONS #############################################
################################################################################

# Permission denied sur /mnt/data1_100g/
sudo chown -R nox\:nox /mnt/data1_100g/agent_llm_local/models/

# Port 11434 d√©j√† utilis√©
sudo fuser -k 11434/tcp                  # Lib√®re le port
ollama serve --port 11435                # Change de port

# Mod√®le corrompu
ollama rm <mod√®le>                       # Supprime le mod√®le corrompu
ollama pull <mod√®le>                      # R√©installe le mod√®le

################################################################################
# üîÑ SCRIPT COMPLET (Bruno Delnoz) ###############################################
################################################################################

#!/bin/bash
# Script: ollama_tools.sh
# Auteur: Bruno Delnoz (nox@casablanca)
# Usage: ./ollama_tools.sh {download|analyze|list|server} [args]

# Fonctions
download_model() { ollama pull "\$1"; }
analyze_file() { ollama run phi3:3.8b "Analyse \$(cat \$1)" > "analyse_\$(date +%Y%m%d).md"; }
list_models() { ollama list; }
start_server() {
  export OLLAMA_MODELS=/mnt/data1_100g/agent_llm_local/models
  ollama serve --host 0.0.0.0 --port 11434
}

# Gestion des arguments
case "\$1" in
  download) download_model "\$2" ;;
  analyze) analyze_file "\$2" ;;
  list) list_models ;;
  server) start_server ;;
  *) echo "Usage: \$0 {download|analyze|list|server} [args]"; exit 1 ;;
esac
