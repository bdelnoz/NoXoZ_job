######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.2_Technical/COMPOSANTS.csv
######################################################################
Composant Principal,Sub‚ÄëComposant / Module,R√¥le / Fonction sp√©cifique,Installation,Configuration,Testing / Cr√©ation,Validation,E2E Validation,D√©tails Techniques / Fichiers & Folders,,
LLM Engine,Ollama (Mistral-7B),Inf√©rence principale, prompt ‚Üí r√©ponse,Done,Done,Done,Done,Todo,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mistral-7b/; ex√©cutable: ollama (/usr/local/bin/ollama); service Ollama local; dossiers: /2_Sources/2.1_Python/llm/,
LLM Engine,Mixtral,Variante LLM pour tests multi-mod√®les,Done,Done,Done,Done,Todo,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mixtral/; ex√©cutable: ollama (/usr/local/bin/ollama); utilis√© pour tests multi-mod√®les; dossiers: /2_Sources/2.1_Python/llm/,,
Document Ingestion,PDF Loader,Parsing PDF, extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/pdf_loader.py; d√©pendances: PyPDF2; dossiers: 2_Sources/2.1_Python/loaders/,
Document Ingestion,DOCX Loader,Parsing DOCX, extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/docx_loader.py; d√©pendances: python-docx; dossiers: 2_Sources/2.1_Python/loaders/,
Document Ingestion,MD Loader,Parsing Markdown,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/md_loader.py; d√©pendances: markdown; dossiers: 2_Sources/2.1_Python/loaders/,,
Document Ingestion,JSON / XML Loader,Parsing fichiers structur√©s,Done,Done,Done,Done,Todo,"Scripts: json_loader.py / xml_loader.py; d√©pendances: xml.etree, json; dossiers: 2_Sources/2.1_Python/loaders/",,
Vector Store,Chroma Vector DB,Stockage embeddings persistants,Done,Done,Done,Done,Todo,Installation via pip install chromadb; stockage: /mnt/data1_100g/agent_llm_local/vectors/; dossiers: 3_Data/3.1_Vectors/,,
Vector Store,SQLite Metadata,Stockage m√©tadonn√©es CV / chats / docs,Done,Done,Done,Done,Todo,Fichier DB: /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/3_Data/Metadata/db.sqlite; script init: init_sqlite.sh; dossiers: 3_Data/3.2_Metadata/,,
Agent Logic,ReAct Logic,Raisonnement bas√© sur donn√©es, ex√©cution de cha√Ænes d‚Äôactions,Done,Done,Done,Done,Todo,Scripts: 2_Sources/2.1_Python/agent_react.py; d√©pendances: LangChain; dossiers: 2_Sources/2.1_Python/agent/,
Agent Logic,Tools / Pipelines,Actions auxiliaires, int√©gration LLM + base,Done,Done,Done,Done,Todo,Dossiers: 2_Sources/2.1_Python/tools/; ex√©cutable: pipelines.py,
API Layer,FastAPI endpoints ‚Äì /upload,Upload fichier via API,Done,Done,Done,Done,Todo,"Scripts: main_agent.py, router.py, status.py; dossiers: 2_Sources/2.1_Python/; endpoint /upload; d√©pendances: fastapi, uvicorn",,
API Layer,FastAPI endpoints ‚Äì /generate,G√©n√©ration document via API,Done,Done,Done,Done,Todo,"Fichiers: main_agent.py; endpoint /generate; d√©pendances: fastapi, uvicorn",,
API Layer,FastAPI endpoints ‚Äì /status,V√©rification statut composants,Done,Done,Done,Done,Todo,"Fichiers: status.py; lecture √©tat Ollama, Chroma, SQLite; dossiers: 2_Sources/2.1_Python/",,
DB,SQLite base,Stockage des m√©tadonn√©es CV / chat / docs,Done,Done,Done,Done,Todo,"Fichier: 3_Data/3.2_Metadata/db.sqlite; scripts: init_sqlite.sh, test_sqlite.sh; dossiers: 3_Data/3.2_Metadata/",,
DB,Chroma embeddings,Stockage des vecteurs de similarit√©,Done,Done,Done,Done,Todo,Stockage: /mnt/data1_100g/agent_llm_local/vectors/; dossiers: 3_Data/3.1_Vectors/,,
Output Generation,Pandoc,Conversion MD ‚Üí PDF/DOCX,Done,Done,Done,Done,Todo,Ex√©cutable: pandoc; templates: 9_Templates/,,
Output Generation,python-docx,G√©n√©ration DOCX depuis templates,Done,Done,Done,Done,Todo,Library Python: python-docx; templates: 9_Templates/DOCX/,,
Deployment,Docker Compose,Conteneurisation LLM + API + DB,Done,Done,Todo,Todo,Todo,Fichier: docker-compose.yml; commande: docker-compose up,,
Deployment,Scripts de d√©ploiement,Lancement et gestion des services,Done,Done,Done,Done,Todo,Scripts: 8_Scripts/8.1_Init/*.sh,,
Storage big files,Mod√®les LLM,Stockage fichiers mod√®les lourds,Done,Done,Done,Done,Todo,Path: /mnt/data1_100g/agent_llm_local/models/,,
Storage big files,Embeddings & vecteurs,Stockage vecteurs et bases volumineuses,Done,Done,Done,Done,Todo,Path: /mnt/data1_100g/agent_llm_local/vectors/,,
Storage project files,Scripts / Logs / Outputs,Stockage projet et fichiers interm√©diaires,Done,Done,Done,Done,Todo,Path: /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/,,
Interface Web locale,Page prompt / input,Saisie prompt utilisateur,Done,Done,Done,Done,Todo,Fichiers: templates HTML/Python; endpoint FastAPI /generate; dossiers: 2_Sources/2.1_Python/templates/,,
Interface Web locale,Dashboard status composants,Affichage √©tat API, Ollama, Chroma,Todo,Todo,Todo,Todo,Todo,Scripts √† cr√©er: dashboard.py; lecture /status API; dossiers: 2_Sources/2.1_Python/dashboard/
Interface Web locale,Multi-upload / batch processing,Upload fichiers multiples,Todo,Todo,Todo,Todo,Todo,Endpoint API /upload avec batch; interface UI √† compl√©ter,,
Logging et S√©curit√©,Logs API / ingestion / g√©n√©ration,Logging complet des actions et prompts,Done,Done,Done,Done,Todo,Logs: 4_Logs/; script: log_manager.py,,
Logging et S√©curit√©,Permissions & isolation agents,Contr√¥le acc√®s, isolation contexte LLM,Done,Done,Done,Done,Todo,"Gestion via scripts init, permissions utilisateurs et groupes",
Init Scripts,config_paths.sh / fix_ollama_group_permissions.sh,Cr√©ation structure, correction permissions Ollama,Done,Done,Done,Done,Todo,Path: 8_Scripts/8.1_Init/,
Init Scripts,ollama-batch-download.sh,T√©l√©chargement batch mod√®les Ollama,Done,Done,Done,Done,Todo,Ex√©cutable: ollama-batch-download.sh; logs: 8_Scripts/8.1_Init/logs/,,
Init Scripts,reset_ollama_service.sh,Reset du service Ollama,Done,Done,Done,Done,Todo,Ex√©cutable: reset_ollama_service.sh,,
FastAPI Scripts,start_fastapi.sh,D√©marrage API avec options --status, --force, --keep,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/start_fastapi.sh; logs: 4_Logs/
FastAPI Scripts,test_fastapi.sh,V√©rification endpoints API,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/test_fastapi.sh,,
SQLite / Chroma Scripts,init_sqlite.sh,Initialisation DB + Chroma,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/init_sqlite.sh; DB: 3_Data/3.2_Metadata/db.sqlite,,
SQLite / Chroma Scripts,test_sqlite.sh,V√©rification insert/retrieve vecteurs,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/test_sqlite.sh,,
Templates,DOCX / MD / PDF,Mod√®les documents professionnels,Done,Done,Done,Done,Todo,Path: 9_Templates/,,
CI/CD,GitHub Actions,Automatisation tests et d√©ploiement,Done,Done,Todo,Todo,Todo,Workflow: .github/workflows/*.yml,,

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.2_Technical/OLLAMA_commandes.md
######################################################################
---
# **üìú Commandes Ollama Compl√®tes (Bruno Delnoz - Kali Linux)**
**Auteur** : Bruno Delnoz (nox@casablanca)
**Date** : 2026-02-05
**Version** : 1.0
**Format** : Markdown (`.md`)
**Emplacement** : `/mnt/data2_78g/Security/scripts/Projects_system/Docs/commandes_ollama.md`
**Contexte** : Scripts pour Kali Linux, format Markdown strict, sans num√©ros de version dans les noms de fichiers.
---

```bash
################################################################################
# 1Ô∏è‚É£ GESTION DES MOD√àLES #######################################################
################################################################################

# T√©l√©chargement/Mise √† jour
ollama pull <mod√®le>                     # Ex: ollama pull phi3:3.8b (128K tokens, id√©al pour 350 questions)
ollama pull --help                        # Affiche l'aide compl√®te
ollama pull <mod√®le> --show-progress      # Affiche la progression du t√©l√©chargement
ollama pull --update <mod√®le>             # Met √† jour un mod√®le existant

# Liste/Suppression
ollama list                              # Liste tous les mod√®les locaux
ollama rm <mod√®le>                       # Supprime un mod√®le sp√©cifique (ex: ollama rm mistral:7b)
ollama rm --all                          # ‚ö†Ô∏è Supprime TOUS les mod√®les locaux (√† utiliser avec prudence)

################################################################################
# 2Ô∏è‚É£ EX√âCUTION ET INTERACTION ###################################################
################################################################################

# Mode Interactif
ollama run <mod√®le>                      # Lance un mod√®le en mode conversationnel (ex: ollama run llama3:8b)
ollama run <mod√®le> "<prompt>"           # Ex√©cute une requ√™te unique (ex: ollama run phi3:3.8b "Explique Docker")

# Options Avanc√©es
ollama run <mod√®le> --verbose             # Active le mode verbeux (logs d√©taill√©s)
ollama run <mod√®le> --temperature 0.8    # Contr√¥le la cr√©ativit√© (0=pr√©cis, 1=cr√©atif)
ollama run <mod√®le> --num-gpu 1           # Utilise 1 GPU pour l'inf√©rence
ollama run <mod√®le> --num-threads 8       # Limite √† 8 threads CPU
ollama run <mod√®le> --mirostat 2          # Am√©liore la coh√©rence des r√©ponses
ollama run <mod√®le> --repeat_penalty 1.2  # R√©duit les r√©p√©titions dans les r√©ponses

################################################################################
# 3Ô∏è‚É£ SERVEUR OLLAMA ###########################################################
################################################################################

# D√©marrage/Arr√™t
ollama serve                             # D√©marre le serveur Ollama (port 11434 par d√©faut)
ollama serve --host 0.0.0.0              # ‚ö†Ô∏è Autorise les connexions externes (attention s√©curit√©)
ollama serve --port 11435                # Change le port par d√©faut
pkill -f ollama                          # Arr√™te le serveur (Kali Linux)

# Configuration
export OLLAMA_MODELS=/mnt/data1_100g/agent_llm_local/models  # Chemin personnalis√© pour les mod√®les
export OLLAMA_HOST=0.0.0.0:11434         # D√©finit l'h√¥te et le port
ollama serve --debug                     # Active le mode debug (logs √©tendus)

################################################################################
# 4Ô∏è‚É£ MODELFILES (Personnalisation) #############################################
################################################################################

# Exemple de Modelfile pour un assistant Kali Linux (√† enregistrer sous kali_assistant.modelfile)
# FROM llama2:7b
# PARAMETER temperature 0.8
# SYSTEM "Expert en Kali Linux, sp√©cialis√© en s√©curit√© et scripts shell. R√©ponds de mani√®re concise et technique."

# Cr√©ation du mod√®le personnalis√©
ollama create kali_assistant -f kali_assistant.modelfile

################################################################################
# 5Ô∏è‚É£ API ET R√âSEAU #############################################################
################################################################################

# Requ√™tes API
curl http://localhost:11434/api/tags      # Liste les mod√®les disponibles via l'API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3:8b", "prompt": "Bonjour, explique-moi comment configurer un pare-feu sous Kali Linux"}'

# S√©curit√© R√©seau
ollama serve --tls-key key.pem --tls-cert cert.pem  # Active le chiffrement TLS

################################################################################
# 6Ô∏è‚É£ PERFORMANCES ET D√âBOGAGE ##################################################
################################################################################

# Optimisation des performances
ollama run <mod√®le> --mirostat 2          # Meilleure coh√©rence des r√©ponses
ollama run <mod√®le> --repeat_penalty 1.2  # R√©duit les r√©p√©titions

# D√©bogage
journalctl -u ollama -f                  # Affiche les logs en temps r√©el (systemd)
ollama run <mod√®le> --verbose             # Active le mode verbeux pour un mod√®le sp√©cifique

################################################################################
# 7Ô∏è‚É£ SCRIPTS PR√äTS √Ä L'EMPLOI ###################################################
################################################################################

# Script 1: Analyser un fichier texte (ex: 350 questions)
#!/bin/bash
# Usage: ./analyse_doc.sh mon_fichier.txt "Ta question"
ollama run phi3:3.8b "Analyse ce fichier : \$(cat \$1) et r√©ponds √† : \$2" > "analyse_\$(date +%Y%m%d).md"

# Script 2: G√©n√©rer une synth√®se
#!/bin/bash
# Usage: ./synthese_doc.sh mon_fichier.txt
ollama run llama3:8b "R√©sume ce document en 5 points cl√©s : \$(cat \$1)" > "synthese_\$(date +%Y%m%d).md"

# Script 3: Lister les mod√®les (filtr√©)
ollama list | grep -E "NAME|phi3:3.8b"     # Filtre un mod√®le sp√©cifique

################################################################################
# 8Ô∏è‚É£ MOD√àLES RECOMMAND√âS (Bruno Delnoz) ########################################
################################################################################

# | Mod√®le          | Taille (Go) | Tokens Max | Usage Recommand√©                          | Commande               |
# |-----------------|-------------|------------|-------------------------------------------|-------------------------|
# | phi3:3.8b       | 2.3          | 128,000    | Documents tr√®s longs (350 questions)       | ollama pull phi3:3.8b   |
# | mistral:7b      | 4.1          | 8,192      | Scripts Kali Linux                       | ollama pull mistral:7b  |
# | codellama:34b   | 20           | 32,000     | Analyse de code                          | ollama pull codellama:34b|
# | dbrx:132b       | 240          | 128,000    | Analyse de tr√®s longs documents          | ollama pull dbrx:132b   |

################################################################################
# 9Ô∏è‚É£ ERREURS COURANTES ET SOLUTIONS #############################################
################################################################################

# Permission denied sur /mnt/data1_100g/
sudo chown -R nox\:nox /mnt/data1_100g/agent_llm_local/models/

# Port 11434 d√©j√† utilis√©
sudo fuser -k 11434/tcp                  # Lib√®re le port
ollama serve --port 11435                # Change de port

# Mod√®le corrompu
ollama rm <mod√®le>                       # Supprime le mod√®le corrompu
ollama pull <mod√®le>                      # R√©installe le mod√®le

################################################################################
# üîÑ SCRIPT COMPLET (Bruno Delnoz) ###############################################
################################################################################

#!/bin/bash
# Script: ollama_tools.sh
# Auteur: Bruno Delnoz (nox@casablanca)
# Usage: ./ollama_tools.sh {download|analyze|list|server} [args]

# Fonctions
download_model() { ollama pull "\$1"; }
analyze_file() { ollama run phi3:3.8b "Analyse \$(cat \$1)" > "analyse_\$(date +%Y%m%d).md"; }
list_models() { ollama list; }
start_server() {
  export OLLAMA_MODELS=/mnt/data1_100g/agent_llm_local/models
  ollama serve --host 0.0.0.0 --port 11434
}

# Gestion des arguments
case "\$1" in
  download) download_model "\$2" ;;
  analyze) analyze_file "\$2" ;;
  list) list_models ;;
  server) start_server ;;
  *) echo "Usage: \$0 {download|analyze|list|server} [args]"; exit 1 ;;
esac

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.2_Technical/structure.md
######################################################################
NoXoZ_job/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml          # Pipeline CI/CD
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ old_versions/           # Anciennes versions des scripts (pour changelog)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ all_my_scripts.old_versions.txt
‚îÇ   ‚îú‚îÄ‚îÄ setup_agent_llm.sh       # Script de setup principal
‚îÇ   ‚îî‚îÄ‚îÄ create-repo.sh          # Script pour cr√©er des d√©p√¥ts Git (√† corriger)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main_agent.py            # Code principal de l'agent LLM
‚îÇ   ‚îú‚îÄ‚îÄ WHY.MD                  # Motivations du projet
‚îÇ   ‚îú‚îÄ‚îÄ README.md                # Documentation utilisateur
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md             # Historique des modifications
‚îÇ   ‚îî‚îÄ‚îÄ INSTALL.MD               # Guide d'installation
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ vectors/                 # Stockage des embeddings (Chroma)
‚îÇ   ‚îî‚îÄ‚îÄ metadata.db              # Base de donn√©es SQLite
‚îú‚îÄ‚îÄ docker-compose.yml           # Configuration Docker
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îî‚îÄ‚îÄ outputs/                     # Fichiers g√©n√©r√©s (CV, lettres, etc.)

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.2_Technical/OLLAMA_all_models_with_token_limits.md
######################################################################
# **Liste Exhaustive des Mod√®les Ollama**
*Derni√®re mise √† jour : 2026-02-05*
*Format : Markdown (pour int√©gration directe dans la documentation technique de Bruno Delnoz)*

---

## **1. Mod√®les G√©n√©raux (Text Generation)**

| **Mod√®le**               | **Taille (Go)** | **Commande `ollama pull`**       | **Tokens Max** | **Usage Recommand√©**                     |
|--------------------------|-----------------|-----------------------------------|----------------|------------------------------------------|
| **phi3:3.8b**            | 2.3             | `ollama pull phi3:3.8b`          | 128,000         | Documents tr√®s longs, contexte √©tendu    |
| **phi3:mini**            | 1.8             | `ollama pull phi3:mini`          | 128,000         | L√©ger et efficace                        |
| **llama3:8b**            | 4.7             | `ollama pull llama3:8b`          | 8,192           | Usage g√©n√©ral, raisonnement              |
| **llama3:70b**           | 42              | `ollama pull llama3:70b`         | 32,000          | Raisonnement profond, t√¢ches complexes    |
| **llama2:7b**            | 3.8             | `ollama pull llama2:7b`          | 4,096           | Usage g√©n√©ral                            |
| **llama2:13b**           | 7.5             | `ollama pull llama2:13b`         | 4,096           | T√¢ches plus complexes                    |
| **llama2:70b**           | 42              | `ollama pull llama2:70b`         | 4,096           | Usage avanc√©                             |
| **mistral:7b**           | 4.1             | `ollama pull mistral:7b`          | 8,192           | Rapidit√©, documents techniques            |
| **mixtral:8x7b**         | 26              | `ollama pull mixtral:8x7b`        | 32,000          | √âquilibre performance/ressources          |
| **mixtral:8x22b**        | 64              | `ollama pull mixtral:8x22b`       | 64,000          | Analyse complexe, multilingue            |
| **gemma:2b**             | 1.4             | `ollama pull gemma:2b`            | 8,192           | L√©ger et efficace                        |
| **gemma:7b**             | 4.8             | `ollama pull gemma:7b`            | 8,192           | Usage g√©n√©ral l√©ger                      |
| **gemma2:9b**            | 5.4             | `ollama pull gemma2:9b`          | 8,192           | Am√©lioration de gemma:7b                  |
| **dbrx:132b**            | 240             | `ollama pull dbrx:132b`          | 128,000         | Analyse de tr√®s longs documents           |
| **tinyllama:1.1b**       | 0.6             | `ollama pull tinyllama:1.1b`      | 2,048           | Tests rapides, environnements limit√©s     |

---

## **2. Mod√®les Sp√©cialis√©s (Code, Multilingue, etc.)**

| **Mod√®le**               | **Taille (Go)** | **Commande `ollama pull`**       | **Tokens Max** | **Usage Recommand√©**                     |
|--------------------------|-----------------|-----------------------------------|----------------|------------------------------------------|
| **codellama:7b**         | 3.8             | `ollama pull codellama:7b`        | 8,192           | Analyse de code                          |
| **codellama:13b**        | 7.5             | `ollama pull codellama:13b`       | 8,192           | Code plus complexe                       |
| **codellama:34b**        | 20              | `ollama pull codellama:34b`       | 32,000          | Code complexe, raisonnement technique    |
| **deepseek-coder:1.3b**   | 0.8             | `ollama pull deepseek-coder:1.3b`  | 4,096           | Code l√©ger                              |
| **deepseek-coder:6.7b**   | 4.0             | `ollama pull deepseek-coder:6.7b`  | 32,000          | Sp√©cialis√© pour le code                  |
| **deepseek-coder:33b**    | 19              | `ollama pull deepseek-coder:33b`   | 32,000          | Code avanc√©                              |
| **qwen:0.5b**            | 0.3             | `ollama pull qwen:0.5b`           | 2,048           | Multilingue l√©ger                       |
| **qwen:1.8b**            | 1.1             | `ollama pull qwen:1.8b`           | 2,048           | Multilingue l√©ger                       |
| **qwen:7b**              | 4.2             | `ollama pull qwen:7b`             | 32,000          | Multilingue (chinois, anglais, fran√ßais) |
| **qwen:14b**             | 8.4             | `ollama pull qwen:14b`            | 32,000          | Multilingue avanc√©                      |
| **qwen:72b**             | 135             | `ollama pull qwen:72b`            | 32,000          | Multilingue avanc√©                      |
| **yi:6b**                | 3.5             | `ollama pull yi:6b`               | 32,000          | Documents techniques                     |
| **yi:34b**               | 20              | `ollama pull yi:34b`              | 32,000          | Analyse approfondie                      |
| **sqlcoder:7b**          | 3.8             | `ollama pull sqlcoder:7b`          | 8,192           | Sp√©cialis√© pour SQL                      |
| **starcoder:3b**         | 1.8             | `ollama pull starcoder:3b`         | 8,192           | Code et d√©veloppement                    |
| **starcoder:7b**         | 4.2             | `ollama pull starcoder:7b`         | 8,192           | Code et d√©veloppement                    |
| **starcoder2:3b**        | 1.8             | `ollama pull starcoder2:3b`        | 8,192           | Code et d√©veloppement                    |
| **starcoder2:7b**        | 4.2             | `ollama pull starcoder2:7b`        | 16,384          | Code et d√©veloppement                    |
| **starcoder2:15b**       | 8.5             | `ollama pull starcoder2:15b`       | 16,384          | Code et d√©veloppement                    |
| **phind-codellama:34b**  | 20              | `ollama pull phind-codellama:34b`  | 32,000          | Code avanc√©                              |

---

## **3. Mod√®les Non Censur√©s (Dolphin)**

| **Mod√®le**               | **Taille (Go)** | **Commande `ollama pull`**       | **Tokens Max** | **Usage Recommand√©**                     |
|--------------------------|-----------------|-----------------------------------|----------------|------------------------------------------|
| **dolphin-llama3:8b**    | 4.7             | `ollama pull dolphin-llama3:8b`   | 8,192           | Conversations libres, non censur√©es     |
| **dolphin-mistral:7b**   | 4.1             | `ollama pull dolphin-mistral:7b`  | 8,192           | Usage g√©n√©ral non censur√©               |
| **dolphin-mixtral:8x7b** | 26              | `ollama pull dolphin-mixtral:8x7b`| 32,000          | Analyse complexe non censur√©e           |

---

## **4. Mod√®les L√©gers (Faible Consommation)**

| **Mod√®le**               | **Taille (Go)** | **Commande `ollama pull`**       | **Tokens Max** | **Usage Recommand√©**                     |
|--------------------------|-----------------|-----------------------------------|----------------|------------------------------------------|
| **tinyllama:1.1b**       | 0.6             | `ollama pull tinyllama:1.1b`      | 2,048           | Tests rapides, environnements limit√©s  |
| **phi:2.7b**             | 1.6             | `ollama pull phi:2.7b`            | 2,048           | Petits projets, rapidit√©                |

---

## **5. Mod√®les Exp√©rimentaux et MoE (Mixture of Experts)**

| **Mod√®le**               | **Taille (Go)** | **Commande `ollama pull`**       | **Tokens Max** | **Usage Recommand√©**                     |
|--------------------------|-----------------|-----------------------------------|----------------|------------------------------------------|
| **mixtral:8x7b**         | 26              | `ollama pull mixtral:8x7b`        | 32,000          | √âquilibre performance/ressources          |
| **mixtral:8x22b**        | 64              | `ollama pull mixtral:8x22b`       | 64,000          | Analyse complexe, multilingue            |

---

## **6. Notes et Recommandations**
### **6.1. Choix du Mod√®le**
- **Pour des documents tr√®s longs** (ex: 350 questions) : **`phi3:3.8b`** (128K tokens) ou **`dbrx:132b`** (128K tokens).
- **Pour du code** : **`deepseek-coder:6.7b`** ou **`codellama:34b`** (32K tokens).
- **Pour un usage l√©ger** : **`tinyllama:1.1b`** ou **`phi:2.7b`**.

### **6.2. Commandes Utiles**
- **Lister les mod√®les t√©l√©charg√©s** :
  ```bash
  ollama list

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.2_Technical/table.csv
######################################################################
ÔªøMod√®le,Taille,Contexte max (tokens),Usage recommand√©,Commande ollama pull
dbrx:132b,132B,128,000,Analyse de tr√®s longs documents, recherche,ollama pull dbrx:132b
mixtral:8x22b,22B (MoE),64,000,T√¢ches complexes, multilingue,ollama pull mixtral:8x22b
llama3:70b,70B,32,000,G√©n√©ration de texte avanc√©e, raisonnement,ollama pull llama3:70b
yi:34b,34B,32,000,Documents techniques, code,ollama pull yi:34b
qwen:72b,72B,32,000,Multilingue (chinois, anglais, fran√ßais),ollama pull qwen:72b
deepseek:67b,67B,32,000,Code, analyse technique,ollama pull deepseek:67b
mixtral:8x7b,7B (MoE),32,000,√âquilibre performance/ressources,ollama pull mixtral:8x7b
llama2:70b,70B,4,096,Usage g√©n√©ral (limit√© par Ollama),ollama pull llama2:70b
mistral:7b,7B,8,192,Rapidit√©, usage local l√©ger,ollama pull mistral:7b
phi3:3.8b,3.8B,128,000,Meilleur rapport taille/contexte,ollama pull phi3:3.8b
gemma2:9b,9B,8,192,L√©ger et efficace,ollama pull gemma2:9b
######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.1_General/ARCHITECTURE.md
######################################################################
# NoXoZ_job

Agent LLM local pour l‚Äôingestion, l‚Äôanalyse et la g√©n√©ration de documents professionnels, **100‚ÄØ% hors cloud**, s√©curis√© et modulaire.

---

## Objectif du projet

NoXoZ_job est con√ßu pour fournir un **agent intelligent local**, capable de :

* Ing√©rer et analyser des donn√©es personnelles (CV, conversations, documents, exports de chats).
* Raisonner sur ces donn√©es via un LLM local puissant.
* G√©n√©rer des documents professionnels personnalis√©s (.docx, .md, .pdf).
* Fonctionner enti√®rement hors ligne, sous contr√¥le total de l‚Äôutilisateur.
* √ätre r√©utilisable dans diff√©rents environnements, y compris VirtualBox.

Le projet met l‚Äôaccent sur **robustesse, modularit√© et durabilit√©**, adapt√© √† un usage personnel ou professionnel sensible.

---

## Principes cl√©s

* Ex√©cution 100‚ÄØ% locale
* Pas de d√©pendances cloud
* Contr√¥le complet des donn√©es par l‚Äôutilisateur
* Architecture modulaire et √©volutive
* Installation et configuration enti√®rement script√©es pour r√©utilisation

---

## Interface Web Locale

Le projet inclut une **interface web locale**, accessible uniquement sur la machine h√¥te.

### Acc√®s

* Adresse fixe : `http://127.0.0.1:11111`
* Accessible uniquement depuis la machine locale

### Fonctionnalit√©s ‚Äì Version initiale (MVP)

* Page web minimaliste avec :

  * Champ de saisie pour le prompt utilisateur
  * Bouton **Send** pour envoyer le prompt √† l‚ÄôAPI FastAPI
  * Affichage dynamique de la r√©ponse g√©n√©r√©e par le LLM

### √âvolutions pr√©vues

* S√©lection dynamique des mod√®les Ollama disponibles
* Upload multi-fichiers pour CV, historiques de chat, documents
* Boutons de contr√¥le des composants :

  * Red√©marrage Ollama
  * Red√©marrage API
  * Rechargement de la base vectorielle
* Statut en temps r√©el des composants : API, Ollama, Chroma
* Historique des interactions pour suivi et audit
* Interface web locale s√©curis√©e, **aucune exposition publique**

---

## Objectifs & Fonctionnalit√©s cl√©s

* **GitHub** : [https://github.com/bdelnoz/NoXoZ_job.git](https://github.com/bdelnoz/NoXoZ_job.git)
* **Local repo** : `/mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job`
* **OLLAMA_MODELS** : `/mnt/data1_100g/agent_llm_local/models`
* 100 % local et offline apr√®s installation
* Ingestion de donn√©es : CV, exports de chats (Le Chat, ChatGPT, Grok, Claude)
* Fichiers support√©s : `.md`, `.docx`, `.pdf`, `.json`, `.xml`
* G√©n√©ration documentaire : lettres de motivation, CV personnalis√©s, emails
* Formats de sortie : `.docx`, `.md`, `.pdf`
* API RESTful FastAPI
* Base de donn√©es : SQLite + Chroma pour les embeddings
* D√©ploiement : local natif, VirtualBox, Docker Compose
* Pipeline CI/CD : GitHub Actions
* Installation et configuration enti√®rement script√©es pour r√©utilisation
* OS cible : Linux Kali / Debian

---

## Composants Cl√©s

| Composant                 | Technologie                                             | R√¥le                                            |
| ------------------------- | ------------------------------------------------------- | ----------------------------------------------- |
| **LLM Engine**            | Ollama (Mistral-7B, Mixtral)                            | Inf√©rence locale du mod√®le de langage           |
| **Document Ingestion**    | LangChain Loaders                                       | Parsing des fichiers (PDF, DOCX, MD, JSON, XML) |
| **Vector Store**          | Chroma                                                  | Stockage persistant des embeddings sur disque   |
| **Agent Logic**           | LangChain (ReAct)                                       | Raisonnement et actions bas√©es sur les donn√©es  |
| **API Layer**             | FastAPI                                                 | Endpoints upload, requ√™tes et g√©n√©ration        |
| **DB**                    | SQLite + Chroma                                         | M√©tadonn√©es utilisateur et stockage vectoriel   |
| **Output Generation**     | Pandoc, python-docx                                     | Conversion et g√©n√©ration de fichiers            |
| **Deployment**            | Docker Compose                                          | Conteneurisation pour local / VirtualBox        |
| **Storage big files**     | `/mnt/data1_100g/agent_llm_local/`                      | Stockage centralis√© des gros volumes            |
| **Storage project files** | `/mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job` | Stockage centralis√© des fichiers du projet      |

---

## Architecture ‚Äì Diagramme (Text-Based)

```text
+-------------+     +-------------+
| User Input  |     | GitHub Repo |
| (CLI / UI)  |<--->| NoXoZ_job   |
+-------------+     +-------------+
         |
         v
 +----------------+        +-------------+
 | FastAPI Server |<------>| CI/CD       |
 +----------------+        | Pipelines   |
         |                 +-------------+
         v
 +-----------------+
 | LangChain Agent |
 | (ReAct + Tools) |
 +-----------------+
         |
         v
 +-----------------+     +----------------------+
 | Ollama LLM      |<--->| Chroma Vector Store  |
 | (Mistral-7B)    |     | + SQLite Metadata   |
 +-----------------+     +----------------------+
         |
         v
 +-----------------+
 | Document Loaders|
 | (.md/.docx/etc) |
 +-----------------+
         |
         v
 +-----------------+
 | Output Generators|
 | (Pandoc / Docx) |
 +-----------------+

Persistent Storage:
- /mnt/data1_100g/agent_llm_local/
- /mnt/data2_78g/.../NoXoZ_job
```

---

## Arborescence du projet

```
NoXoZ_job/
‚îú‚îÄ‚îÄ 1_Documentation/
‚îÇ   ‚îú‚îÄ‚îÄ 1.1_General/
‚îÇ   ‚îî‚îÄ‚îÄ 1.2_Technical/
‚îú‚îÄ‚îÄ 2_Sources/
‚îÇ   ‚îú‚îÄ‚îÄ 2.1_Python/
‚îÇ   ‚îî‚îÄ‚îÄ 2.2_Bash/
‚îú‚îÄ‚îÄ 3_Data/
‚îÇ   ‚îú‚îÄ‚îÄ 3.1_Vectors/
‚îÇ   ‚îî‚îÄ‚îÄ 3.2_Metadata/
‚îú‚îÄ‚îÄ 4_Logs/
‚îú‚îÄ‚îÄ 5_Outputs/
‚îÇ   ‚îú‚îÄ‚îÄ 5.1_DOCX/
‚îÇ   ‚îî‚îÄ‚îÄ 5.2_PDF/
‚îú‚îÄ‚îÄ 6_Results/
‚îÇ   ‚îú‚îÄ‚îÄ 6.1_Bugs/
‚îÇ   ‚îî‚îÄ‚îÄ 6.2_Innovations/
‚îú‚îÄ‚îÄ 7_Infos/
‚îú‚îÄ‚îÄ 8_Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ 8.1_Init/
‚îÇ   ‚îî‚îÄ‚îÄ 8.2_Utils/
‚îú‚îÄ‚îÄ 9_Templates/
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ ARCHITECTURE.md
```

---

## Pipeline Fonctionnel

1. **Ingestion** : Upload et parsing des fichiers (CV, chats, docs) via API ou interface web.
2. **Analyse** : Extraction des informations cl√©s avec NLP (comp√©tences, exp√©riences).
3. **G√©n√©ration** : Templates et logique LLM pour cr√©er documents (CV, lettres, emails).
4. **Export** : Formats `.docx`, `.md`, `.pdf`.

---

## S√©curit√© et Contr√¥le

* Contr√¥le d‚Äôacc√®s utilisateur et r√¥le
* Isolation logique des agents pour √©viter fuites de contexte
* Logging complet : prompts, r√©ponses, mod√®les utilis√©s, actions
* Aucune exposition r√©seau externe par d√©faut

---

## D√©ploiement rapide (r√©sum√©)

1. Installer un Linux propre (VM ou machine d√©di√©e)
2. Installer Python, Ollama, d√©pendances
3. Lancer Ollama et charger les mod√®les
4. D√©marrer l‚ÄôAPI FastAPI
5. Acc√©der √† l‚Äôinterface web sur `127.0.0.1:11111`

---

## √âvolutions pr√©vues

* Interface web avanc√©e avec multi-mod√®les
* Upload multi-fichiers et batch processing
* Multi-agents sp√©cialis√©s et orchestration avanc√©e
* Matching automatique CV / offres
* M√©moire long terme enrichie pour suivi historique
* Tableau de bord avec statut en temps r√©el des composants

---

**Auteur** : Bruno Delnoz
**Date** : 04/02/2026

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.1_General/ollama-models-guide.md
######################################################################
# GUIDE RAPIDE MOD√àLES OLLAMA

## MISTRAL (4.1GB chacun)
**mistral:latest** | **mistral:7b**
- Usage: G√©n√©ral fran√ßais/anglais, conversations, r√©daction
- Fort: Excellent en fran√ßais, raisonnement solide
- Commande: `ollama run mistral "ta question"`

## LLAMA 3.2 (2GB chacun)
**llama3.2:latest** | **llama3.2:3b**
- Usage: Conversations rapides, t√¢ches l√©g√®res
- Fort: Rapide, peu gourmand, bon g√©n√©raliste
- Commande: `ollama run llama3.2 "ta question"`

## CODELLAMA (3.8GB)
**codellama:7b**
- Usage: G√©n√©ration code, debug, explications techniques
- Fort: Python, JavaScript, bash, SQL
- Commande: `ollama run codellama "√©cris fonction python pour..."`

## GEMMA2 (1.6GB)
**gemma2:2b**
- Usage: Petit, rapide, multit√¢che basique
- Fort: Efficacit√©/taille, conversations courtes
- Commande: `ollama run gemma2:2b "question simple"`

## QWEN 2.5 (1.9GB)
**qwen2.5:3b**
- Usage: Multilingue, maths, raisonnement
- Fort: Chinois/anglais, calculs, logique
- Commande: `ollama run qwen2.5:3b "probl√®me maths"`

## DEEPSEEK-R1 (4.4GB et 4.7GB)
**deepseek-r1:7b** | **deepseek-r1:8b**
- Usage: Raisonnement complexe, r√©solution probl√®mes
- Fort: Analyse √©tape par √©tape, logique avanc√©e
- Commande: `ollama run deepseek-r1:7b "raisonne sur..."`

## PHI3 (2.3GB et 7.9GB)
**phi3:mini** | **phi3:medium**
- Usage: Efficace qualit√©/taille, t√¢ches vari√©es
- Fort: Mini=rapide, Medium=pr√©cis
- Commande: `ollama run phi3:mini "question"`

## ORCA-MINI (1.9GB)
**orca-mini:3b**
- Usage: Conversations naturelles compactes
- Fort: Dialogues fluides, peu de ressources
- Commande: `ollama run orca-mini "discutons de..."`

## NEURAL-CHAT (4.1GB)
**neural-chat:7b**
- Usage: Chat optimis√©, assistant conversationnel
- Fort: R√©ponses structur√©es, tonalit√© adapt√©e
- Commande: `ollama run neural-chat "aide-moi √†..."`

## STARLING-LM (4.1GB)
**starling-lm:7b**
- Usage: Chat haute qualit√©, instructions pr√©cises
- Fort: Suivi instructions, r√©ponses d√©taill√©es
- Commande: `ollama run starling-lm "explique..."`

## SOLAR (6.1GB)
**solar:10.7b**
- Usage: Raisonnement avanc√©, t√¢ches complexes
- Fort: Plus gros mod√®le, meilleures performances
- Commande: `ollama run solar "analyse complexe..."`

---

## CONSEILS UTILISATION

**Pour coder**: codellama, deepseek-r1
**Pour fran√ßais**: mistral
**Pour rapidit√©**: gemma2, llama3.2, phi3:mini
**Pour qualit√©**: solar, phi3:medium, starling-lm
**Pour raisonnement**: deepseek-r1, solar, qwen2.5
**Pour conversations**: neural-chat, orca-mini, mistral

**Commande g√©n√©rique**:
```bash
ollama run <modele> "ta question ou prompt"
```

**Lister mod√®les install√©s**:
```bash
ollama list
```

######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.3_Status/plan_detaille.md
######################################################################

# Plan D√©taill√© des √âtapes Techniques
*Bruno Delnoz ‚Äì Projet LLM & Document Ingestion*
*Derni√®re mise √† jour : 07/02/2026*

---

## **1. LLM Engine & Document Ingestion**
### **1.1. Ollama (Mistral-7B)**
- **V√©rification du service**
  ```bash
  systemctl status ollama.service
  sudo systemctl start ollama.service
  journalctl -u ollama.service -f
  ```
- **Test de g√©n√©ration**
  ```bash
  ollama run mistral-7b "Test prompt"
  ```
- **V√©rification des mod√®les**
  ```bash
  ollama list
  ls /mnt/data1_100g/agent_llm_local/models/mistral-7b/
  ```
- **R√©solution des d√©pendances**
  ```bash
  ollama pull mistral-7b
  ldd $(which ollama)
  ```

### **1.2. Mixtral**
- **Compatibilit√© multi-mod√®les**
  ```bash
  ollama run mixtral
  ```
- **Tests de g√©n√©ration**
  ```bash
  ollama run mixtral "Test parallel prompt"
  ```

### **1.3. Document Loaders**
- **JSON/XML Loader**
  ```bash
  python test_json_loader.py
  ```
- **MD/DOCX/PDF Loader**
  ```bash
  python test_md_loader.py
  python test_docx_loader.py
  python test_pdf_loader.py
  ```

---

## **2. Vector Store**
### **2.1. Chroma Vector DB**
- **Initialisation**
  ```bash
  chroma run --path /mnt/data1_100g/chroma_db
  ```
- **Tests de persistance**
  ```bash
  python test_chroma_persistence.py
  ```

### **2.2. SQLite Metadata**
- **Initialisation**
  ```bash
  bash init_sqlite.sh
  ```
- **Tests de r√©cup√©ration**
  ```bash
  python test_sqlite_metadata.py
  ```

---

## **3. Agent Logic (ReAct + Pipelines)**
- **Compl√©ter le code de d√©cision/action**
  ```bash
  python agent_logic.py
  ```
- **Tests unitaires**
  ```bash
  python test_react_logic.py
  ```

---

## **4. API Layer (FastAPI)**
- **V√©rification des endpoints**
  ```bash
  curl http://localhost:8000/upload
  curl http://localhost:8000/generate
  ```
- **Tests de charge**
  ```bash
  bash test_fastapi.sh
  ```

---

## **5. Output Generation**
- **Installation des d√©pendances**
  ```bash
  sudo apt install pandoc
  pip install python-docx
  ```
- **Tests de g√©n√©ration**
  ```bash
  python test_output_generation.py
  ```

---

## **6. Interface Web locale**
- **Int√©gration avec l‚ÄôAPI**
  ```bash
  python frontend/index.html
  ```
- **Dashboard**
  ```bash
  python dashboard.py
  ```

---

## **7. Logging & S√©curit√©**
- **Finaliser log_manager.py**
  ```bash
  python log_manager.py
  ```
- **Permissions**
  ```bash
  chown -R user:group /mnt/data1_100g/
  chmod 700 /mnt/data1_100g/agent_llm_local/
  ```

---

## **8. Deployment**
- **Docker Compose**
  ```bash
  docker-compose up -d
  ```
- **Scripts d‚Äôinitialisation**
  ```bash
  bash start_all.sh
  ```

---

## **9. Templates & CI/CD**
- **Cr√©ation des mod√®les**
  ```bash
  python create_templates.py
  ```
- **Workflow CI/CD**
  ```yaml
  # .github/workflows/test_deploy.yml
  name: Test & Deploy
  on: [push]
  jobs:
    test:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v2
        - run: python -m pytest
  ```

---

# Annexe : CSV des Composants
*Mise √† jour : 07/02/2026*

```csv
Composant Principal,Sub‚ÄëComposant / Module,R√¥le / Fonction sp√©cifique,Installation,Configuration,Testing / Cr√©ation,Validation,E2E Validation,D√©tails Techniques / Fichiers & Folders
LLM Engine,Ollama (Mistral-7B),Inf√©rence principale,prompt ‚Üí r√©ponse,Done,Done,Done,Done,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mistral-7b/
LLM Engine,Mixtral,Variante LLM pour tests multi-mod√®les,Done,Done,Done,Todo,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mixtral/
Document Ingestion,PDF Loader,Parsing PDF,extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Script: 2_Sources/2.1_Python/loaders/pdf_loader.py
Document Ingestion,DOCX Loader,Parsing DOCX,extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Script: 2_Sources/2.1_Python/loaders/docx_loader.py
Document Ingestion,MD Loader,Parsing Markdown,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/md_loader.py
```


######################################################################
# FULLPATH /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/1_Documentation/1.3_Status/COMPOSANTS_mis_a_jour.csv
######################################################################
Composant Principal,Sub‚ÄëComposant / Module,R√¥le / Fonction sp√©cifique,Installation,Configuration,Testing / Cr√©ation,Validation,E2E Validation,D√©tails Techniques / Fichiers & Folders,Unnamed: 9,Unnamed: 10
LLM Engine,Ollama (Mistral-7B),Inf√©rence principale, prompt ‚Üí r√©ponse,Done,Done,Done,Done,Todo,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mistral-7b/; ex√©cutable: ollama (/usr/local/bin/ollama); service Ollama local; dossiers: /2_Sources/2.1_Python/llm/,
LLM Engine,Mixtral,Variante LLM pour tests multi-mod√®les,Done,Done,Done,Done,Todo,Mod√®les: /mnt/data1_100g/agent_llm_local/models/mixtral/; ex√©cutable: ollama (/usr/local/bin/ollama); utilis√© pour tests multi-mod√®les; dossiers: /2_Sources/2.1_Python/llm/,,
Document Ingestion,PDF Loader,Parsing PDF, extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/pdf_loader.py; d√©pendances: PyPDF2; dossiers: 2_Sources/2.1_Python/loaders/,
Document Ingestion,DOCX Loader,Parsing DOCX, extraction texte et m√©tadonn√©es,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/docx_loader.py; d√©pendances: python-docx; dossiers: 2_Sources/2.1_Python/loaders/,
Document Ingestion,MD Loader,Parsing Markdown,Done,Done,Done,Done,Todo,Script: 2_Sources/2.1_Python/loaders/md_loader.py; d√©pendances: markdown; dossiers: 2_Sources/2.1_Python/loaders/,,
Document Ingestion,JSON / XML Loader,Parsing fichiers structur√©s,Done,Done,Done,Done,Todo,"Scripts: json_loader.py / xml_loader.py; d√©pendances: xml.etree, json; dossiers: 2_Sources/2.1_Python/loaders/",,
Vector Store,Chroma Vector DB,Stockage embeddings persistants,Done,Done,Done,Done,Todo,Installation via pip install chromadb; stockage: /mnt/data1_100g/agent_llm_local/vectors/; dossiers: 3_Data/3.1_Vectors/,,
Vector Store,SQLite Metadata,Stockage m√©tadonn√©es CV / chats / docs,Done,Done,Done,Done,Todo,Fichier DB: /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/3_Data/Metadata/db.sqlite; script init: init_sqlite.sh; dossiers: 3_Data/3.2_Metadata/,,
Agent Logic,ReAct Logic,Raisonnement bas√© sur donn√©es, ex√©cution de cha√Ænes d‚Äôactions,Done,Done,Done,Done,Todo,Scripts: 2_Sources/2.1_Python/agent_react.py; d√©pendances: LangChain; dossiers: 2_Sources/2.1_Python/agent/,
Agent Logic,Tools / Pipelines,Actions auxiliaires, int√©gration LLM + base,Done,Done,Done,Done,Todo,Dossiers: 2_Sources/2.1_Python/tools/; ex√©cutable: pipelines.py,
API Layer,FastAPI endpoints ‚Äì /upload,Upload fichier via API,Done,Done,Done,Done,Todo,"Scripts: main_agent.py, router.py, status.py; dossiers: 2_Sources/2.1_Python/; endpoint /upload; d√©pendances: fastapi, uvicorn",,
API Layer,FastAPI endpoints ‚Äì /generate,G√©n√©ration document via API,Done,Done,Done,Done,Todo,"Fichiers: main_agent.py; endpoint /generate; d√©pendances: fastapi, uvicorn",,
API Layer,FastAPI endpoints ‚Äì /status,V√©rification statut composants,Done,Done,Done,Done,Todo,"Fichiers: status.py; lecture √©tat Ollama, Chroma, SQLite; dossiers: 2_Sources/2.1_Python/",,
DB,SQLite base,Stockage des m√©tadonn√©es CV / chat / docs,Done,Done,Done,Done,Todo,"Fichier: 3_Data/3.2_Metadata/db.sqlite; scripts: init_sqlite.sh, test_sqlite.sh; dossiers: 3_Data/3.2_Metadata/",,
DB,Chroma embeddings,Stockage des vecteurs de similarit√©,Done,Done,Done,Done,Todo,Stockage: /mnt/data1_100g/agent_llm_local/vectors/; dossiers: 3_Data/3.1_Vectors/,,
Output Generation,Pandoc,Conversion MD ‚Üí PDF/DOCX,Done,Done,Done,Done,Todo,Ex√©cutable: pandoc; templates: 9_Templates/,,
Output Generation,python-docx,G√©n√©ration DOCX depuis templates,Done,Done,Done,Done,Todo,Library Python: python-docx; templates: 9_Templates/DOCX/,,
Deployment,Docker Compose,Conteneurisation LLM + API + DB,Done,Done,Todo,Todo,Todo,Fichier: docker-compose.yml; commande: docker-compose up,,
Deployment,Scripts de d√©ploiement,Lancement et gestion des services,Done,Done,Done,Done,Todo,Scripts: 8_Scripts/8.1_Init/*.sh,,
Storage big files,Mod√®les LLM,Stockage fichiers mod√®les lourds,Done,Done,Done,Done,Todo,Path: /mnt/data1_100g/agent_llm_local/models/,,
Storage big files,Embeddings & vecteurs,Stockage vecteurs et bases volumineuses,Done,Done,Done,Done,Todo,Path: /mnt/data1_100g/agent_llm_local/vectors/,,
Storage project files,Scripts / Logs / Outputs,Stockage projet et fichiers interm√©diaires,Done,Done,Done,Done,Todo,Path: /mnt/data2_78g/Security/scripts/AI_Projects/NoXoZ_job/,,
Interface Web locale,Page prompt / input,Saisie prompt utilisateur,Done,Done,Done,Done,Todo,Fichiers: templates HTML/Python; endpoint FastAPI /generate; dossiers: 2_Sources/2.1_Python/templates/,,
Interface Web locale,Dashboard status composants,Affichage √©tat API, Ollama, Chroma,Todo,Todo,Todo,Todo,Todo,Scripts √† cr√©er: dashboard.py; lecture /status API; dossiers: 2_Sources/2.1_Python/dashboard/
Interface Web locale,Multi-upload / batch processing,Upload fichiers multiples,Todo,Todo,Todo,Todo,Todo,Endpoint API /upload avec batch; interface UI √† compl√©ter,,
Logging et S√©curit√©,Logs API / ingestion / g√©n√©ration,Logging complet des actions et prompts,Done,Done,Done,Done,Todo,Logs: 4_Logs/; script: log_manager.py,,
Logging et S√©curit√©,Permissions & isolation agents,Contr√¥le acc√®s, isolation contexte LLM,Done,Done,Done,Done,Todo,"Gestion via scripts init, permissions utilisateurs et groupes",
Init Scripts,config_paths.sh / fix_ollama_group_permissions.sh,Cr√©ation structure, correction permissions Ollama,Done,Done,Done,Done,Todo,Path: 8_Scripts/8.1_Init/,
Init Scripts,ollama-batch-download.sh,T√©l√©chargement batch mod√®les Ollama,Done,Done,Done,Done,Todo,Ex√©cutable: ollama-batch-download.sh; logs: 8_Scripts/8.1_Init/logs/,,
Init Scripts,reset_ollama_service.sh,Reset du service Ollama,Done,Done,Done,Done,Todo,Ex√©cutable: reset_ollama_service.sh,,
FastAPI Scripts,start_fastapi.sh,D√©marrage API avec options --status, --force, --keep,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/start_fastapi.sh; logs: 4_Logs/
FastAPI Scripts,test_fastapi.sh,V√©rification endpoints API,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/test_fastapi.sh,,
SQLite / Chroma Scripts,init_sqlite.sh,Initialisation DB + Chroma,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/init_sqlite.sh; DB: 3_Data/3.2_Metadata/db.sqlite,,
SQLite / Chroma Scripts,test_sqlite.sh,V√©rification insert/retrieve vecteurs,Done,Done,Done,Done,Todo,Script: 8_Scripts/8.1_Init/test_sqlite.sh,,
Templates,DOCX / MD / PDF,Mod√®les documents professionnels,Done,Done,Done,Done,Todo,Path: 9_Templates/,,
CI/CD,GitHub Actions,Automatisation tests et d√©ploiement,Done,Done,Todo,Todo,Todo,Workflow: .github/workflows/*.yml,,

